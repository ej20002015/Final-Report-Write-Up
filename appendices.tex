\begin{appendices}

%
% The first appendix must be "Self-appraisal".
%
\chapter{Self-appraisal}

\section{Critical Self-Evaluation}

Overall, I am extremely satisfied with the quality of my project. Highlighted below are several positive aspects in which my project excels, as well as a few areas for improvement.

\subsection{Positives}

Perhaps the strongest part of my report is the Background Research. This section provides an extremely thorough review of Blinn-Phong and Physically Based Shading. Of particular achievement is the way in which the theory behind PBS is successively built up, starting from the foundational physics of light-matter interaction, and culminating in the examination of specific physically based shading models. This research equips the reader with a rigorous understanding of the subject matter, which pays dividends in the later chapters. Crucially, the Background Research is informed by an excellent set of literature that includes many of the major pioneering papers within the realms of real-time rendering and shading models. Furthermore, where appropriate, papers from other disciplines were utilised, such as those from optics and spectroscopy.

My report also boasts a well thought out Methods chapter. Sections \ref{BlinnPhongImplementation} and \ref{PBRImplementation} give a detailed explanation of how the two renderers were implemented, and also meticulously justify all the design decisions that are made. Select areas are especially accomplished, such as the explanation of Hammon's approximation.

Another strong facet of my project is the implementation. The code base is clean and well designed. All function and variable identifies are given meaningful names, and comments are used where necessary, and never as a substitute for good code. Special care is given to each of the functions and methods to ensure that they only fulfil one purpose, and operate at a single level of abstraction. Errors are handled appropriately using a combination of asserts and exceptions. As is good practice with C++, smart pointers are employed throughout, which help to eliminate any memory leaks and segmentation faults. Notable aspects of the architecture are the rendering API, and the APIs for creating scenes. Despite two distinct renderers operating in the background, a client can access all the rendering commands through a single unified interface. Furthermore, the APIs for creating models and point lights, and then adding them to a scene, are all very intuitive.

As my project progressed, it became clear that there were not many physical phenomena that could be objectively measured and used in my Results chapter. With this in mind, the fact that I was able to identify and systematically investigate two phenomena is positive. Moreover, the evidence gathered in the Results chapter is indicative of the realities of using the two shading models, since either of the renderers developed could be easily migrated into a real 3D interactive application, such as a game engine or piece of modelling software.

My report features a high quality of writing throughout. Figures and tables are used correctly, and referenced in the relevant parts of the main text, and the language is of standard that is suitable for academic papers and technical reports. Particular focus is placed on having the report progress along a consistent narrative. By utilising section introductions, cross referencing, and a coherent structure, the reader is guided through the text. There is no ambiguity about the content of each section, and the purpose it serves in the overall report.

\subsection{Areas for Improvement}

The weakest chapter of my report is the Results. Although the quality is still of a decent standard, and the testing and analysis is scientific, there is certainly room for improvement. Instead of relegating the discussion of the MERL BRDF database and Image Slices to the Future Work Section, they could form a prominent part of the Background Research, and then subsequently be used to compare Blinn-Phong shading to PBS. This would result in a more objective evaluation strategy. However, it is worth noting that this would also fundamentally change the project: the focus would be on extracting a BRDF from the Blinn-Phong shading model and developing a BRDF viewer application, rather than creating two real-time renderers. An alternative way of bolstering the Results chapter would be to gather more quantitative results by means of user research. A study could be performed that asks users to compare frames and rate their perceived realism. Of course, the subjectivity of such an approach is warned against in the Introduction, but if performed scientifically and averaged over enough people, then the resulting data would be valid to draw conclusions from.

Another significant shortcoming of my project is the lack of a sophisticated method for computing indirect illumination. Only utilising a crude ambient term hinders the quality of the physically based renderer. Extending it with Image Based Lighting and Irradiance Mapping techniques would allow the renderer to reach its full potential. Furthermore, as explained in the Future Work Section, these enhancements would broaden the scope of physical phenomena that could then be investigated. This could potentially lead to further improvements in the Results chapter. Clearly, given the strict time pressures, the feasibility of implementing such features is arguable though.

\section{Personal Reflection and Lessons Learned}

Reflecting back on the project process, I am very happy with how everything went, and am especially pleased with the knowledge I accrued. Real-time rendering was already an area of computer science I was passionate about prior to beginning the project; being able to build on this by exploring cutting edge technologies like physically based shading, was incredibly rewarding.

The most demanding part of the entire project process was the write up. Although I consider myself to be a component writer, albeit it slow, I had never undertaken a technical report of this manner. Thankfully, after countless hours and many re-writes, I've been able to produce a report that I'm proud of. As part of this process, I learnt about many aspects that contribute to a good technical report. These include: how to structure the text; the language and grammar that is appropriate; the correct way to cross-reference, and reference figures and tables; and finally, how to cite external sources.

Writing the project was made substantially easier by employing Latex. Initially, I was reluctant to use it, since I had no previous experience - the last thing I wanted to be doing whilst completing the project was also learning a new tool. However, due to the complex mathematical equations that I needed to write, I decided it was my best option. Fortunately, I found Latex to be quite intuitive, and the professional look of the outputted text easily outweighed the slight learning curve. Suffice to say, Latex will be my first choice when creating documents in the future.

Another key skill I developed over the course of the project was how to read academic papers. Interpreting the content of the papers became easier as I grew accustom to the subject matter, but more significantly, I learnt how to skim the papers to find the items relevant to my project. This helped greatly when I was compiling my bibliography.

\section{Legal, Social, Ethical and Professional Issues}


\subsection{Legal Issues}

The legal issues that need to be considered for my project are purely centred around the use of external materials.

As previously mentioned, I've made use of a number of libraries in my implementation, all of which are given in Appendix \ref{SoftwareLibraries}. In order to comply with licensing requirements, the licenses for each library are included in the code base, and it is made clear that these are not my own work.

Appendix \ref{AssetsUsed} lists graphical assets, like textures, that are used in this project. All assets are licensed under CC0, meaning they are owned by the public. As such, there is no need to link to any licenses or the original author, although this is still done in Appendix \ref{AssetsUsed} as a courtesy. The only 'restriction' CC0 places, is to make it clear that these assets are not owned by myself: they are not.

\subsection{Social Issues}

There are no social issues that need to be considered for my project. I did not conduct any user surveys, and the software produced could not be used to create social disorder.

\subsection{Ethical Issues}

There are no ethical issues to be considered for my project. The subject matter is not sensitive, and there is no way in which the findings of this report, or the implementation, could be used in an unethical manner.

\subsection{Professional Issues}

There are no professional issues that need to be considered for my project. All work undertaken is completely personal, and is not intended to be used by a company or anybody else. That being said, I still wrote code of a professional level.

\chapter{External Material} \label{ExternalMaterial}

Three types of external material were used in my solution: software libraries, an implementation of the ACES tone mapping operator, and graphical assets.

\section{Software Libraries} \label{SoftwareLibraries}

The libraries used in my renderer are listed in Table \ref{tb:Libraries}. For each one, a description is provided, along with a link. Note that the separation between my code and the external libraries is very clear in the code base, with all libraries being present under the \mintinline{bash}|Application/Vendor| directory.

\begin{table}[ht!]
	\begin{tabular}{|m{5em}|m{25em}|m{8em}|}
		\hline
		\textbf{Library Name} & \textbf{Description} & \textbf{Link} \\
		\hline\hline
		GLFW & A cross-platform utility library that provides windowing, OpenGL contexts and retrieves input events & \url{https://www.glfw.org/} \\
		\hline
		Glad & A library for loading pointers to the OpenGL functions & \url{https://glad.dav1d.de/} \\
		\hline
		GLM	& A maths library specifically designed for use with OpenGL & \url{https://github.com/g-truc/glm} \\
		\hline
		spdlog & A fast logging library & \url{https://github.com/gabime/spdlog} \\
		\hline
		stb\_image & An image loading library used when creating textures & \url{https://github.com/nothings/stb/blob/master/stb_image.h} \\
		\hline
		assimp & A library to import 3D models of various different file types & \url{https://github.com/assimp/assimp} \\
		\hline
	\end{tabular}
	\caption{Libraries used in the implementation}
	\label{tb:Libraries}
\end{table}

\section{ACES Tone Mapping Operator} \label{ACESExternalMaterial}

The implementation of the ACES tone mapping operator, which is given in Listing \ref{ls:ACES}, is a slightly modified version of Stephen Hill's code. His code can be accessed via the following link: \url{https://github.com/TheRealMJP/BakingLab/blob/master/BakingLab/ACES.hlsl}

\section{Assets} \label{AssetsUsed}

The only external graphical assets used in my project are a couple of textures. These are listed in Table \ref{tb:AssetsUsed}, with each entry giving a location of the asset in the code base, as well as a link to where it was taken from.

\begin{table}
	\begin{tabular}{|m{19em}|m{19em}|}
		\hline
		\textbf{Asset Path} & \textbf{Link} \\
		\hline\hline
		\url{Application/Assets/Textures/ParticleBoard/ParticleBoardBaseColor.jpg} & \url{https://3dtextures.me/2021/08/11/wood-particle-board-003/} \\
		\hline
		\url{Application/Assets/Textures/WoodenFloor/WoodenFloorBaseColor.jpg} & \url{https://3dtextures.me/2018/02/14/wood-floor-007/} \\
		\hline
	\end{tabular}
	\caption{Graphical assets used in the implementation}
	\label{tb:AssetsUsed}
\end{table}

\chapter{Mathematical Notation} \label{MathematicalNotation}

\begin{center}
	\makebox[\textwidth][c]{
	\begin{tabular}{ c c }
		\hline
		\begin{math}\vect{n}\end{math} & Normal vector \\
		\begin{math}\vect{l}\end{math} & Light direction \\
		\begin{math}\vect{v}\end{math} & View vector \\
		\begin{math}\vect{h}\end{math} & Half vector \\
		\begin{math}\vect{a}\cdot\vect{b}\end{math} & The dot product of vectors \begin{math}\vect{a}\end{math} and \begin{math}\vect{b}\end{math} \\
		\begin{math}\norm{\vect{a}}\end{math} & The norm of vector \begin{math}\vect{a}\end{math} \\
		\begin{math}\abs{x}\end{math} & The absolute value of \begin{math}x\end{math} \\
		\begin{math}x^+\end{math} & Clamp \begin{math}x\end{math} to \begin{math}0\end{math} if \begin{math}x<0\end{math} \\
		\begin{math}\mathcal{X}^+(x)\end{math} & Returns 1 if \begin{math}x > 0\end{math}, else returns 0 \\
		\begin{math}lerp(x, y, t)\end{math} & Linearly interpolates between \begin{math}x\end{math} and \begin{math}y\end{math} by the interpolant \begin{math}t\end{math}: \begin{math}lerp(x, y, t) = x(1 - t) + yt\end{math} \\
		\hline
	\end{tabular}
	}
\end{center}

\chapter{Test Scenes} \label{TestScenesInfo}

Each of the three tests carried out in Chapter \ref{chapter3} has corresponding code for creation of their scenes. All of this code is located in the \mintinline{bash}|Application/src/TestScenes| directory. Table \ref{tb:TestSceneMapping} maps each test to files in that directory that contain the associated code.

\begin{table}[ht!]
	\makebox[\textwidth][c]{
	\begin{tabular}{|m{15.5em}|m{15.5em}|}
		\hline
		\textbf{Test} & \textbf{Associated Files} \\
		\hline\hline
		Fresnel Effect Comparison & \mintinline{bash}|FresnelTestScene.h| \mintinline{bash}|FresnelTestScene.cpp| \\
		\hline
		Multiple Lights Comparison & \mintinline{bash}|HDRTestScene.h| \mintinline{bash}|HDRTestScene.cpp| \\
		\hline
		Real-Time & \mintinline{bash}|FrameTimeTestScene.h| \mintinline{bash}|FrameTimeTestScene.cpp| \\
		\hline
	\end{tabular}
	}
	\caption{Code for test scene creation}
	\label{tb:TestSceneMapping}
\end{table}

\chapter{Shader Code} \label{ShaderCode}

All the shader code can be accessed in the code base under the \mintinline{bash}|Application/Assets/Shaders| directory. The code is also given below as it is useful to be able to view the end-to-end process of shading. As is required by most graphics APIs, each shader program given below is split into a vertex and fragment shader (also known as a pixel shader).

\section{Blinn-Phong Shader}

\subsection{Vertex Shader}

\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
linenos,
breaklines
]
{glsl}
#version 460 core

// ATTRIBUTES

layout (location = 0) in vec3 a_position;
layout (location = 1) in vec3 a_normal;
layout (location = 2) in vec2 a_textureCoordinates;
layout (location = 3) in vec3 a_tangent;
layout (location = 4) in vec3 a_bitangent;

// UNIFORMS

uniform mat4 u_transform;
uniform mat4 u_projectionViewMatrix;

// OUTPUTS

struct VertexOutput
{
	vec3 worldPosition;
	vec3 normal;
	vec2 textureCoordinates;
	mat3 TBN;
};

out VertexOutput vertex_output;

// FUNCTIONS

void main()
{
	vertex_output.worldPosition = vec3(u_transform * vec4(a_position, 1.0f));
	vertex_output.normal = normalize(vec3(transpose(inverse(u_transform)) * vec4(a_normal, 0.0f)));
	vertex_output.textureCoordinates = a_textureCoordinates;
	
	vec3 normalTransformed = normalize(vec3(u_transform * vec4(a_normal, 0.0f)));
	vec3 tangentTransformed = normalize(vec3(u_transform * vec4(a_tangent, 0.0f)));
	vec3 bitangentTransformed = normalize(vec3(u_transform * vec4(a_bitangent, 0.0f)));
	vertex_output.TBN = mat3(tangentTransformed, bitangentTransformed, normalTransformed);
	
	gl_Position = u_projectionViewMatrix * u_transform * vec4(a_position, 1.0f);
}
\end{minted}

\subsection{Fragment Shader}

\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
linenos,
breaklines
]
{glsl}
#version 460 core

// INPUTS FROM VERTEX SHADER

struct VertexOutput
{
	vec3 worldPosition;
	vec3 normal;
	vec2 textureCoordinates;
	mat3 TBN;
};

in VertexOutput vertex_output;

// UNIFORMS

// Material

struct Material
{
	vec4 diffuseColor;
	vec3 specularColor;
	sampler2D diffuseMap;
	sampler2D specularMap;
	sampler2D normalMap;
	float shininess;
	
	bool useNormalMap;
};

uniform Material u_material;

// Point lights

struct PointLight
{
	vec3 worldPosition;
	vec3 diffuseComponent;
	vec3 specularComponent;
	float lightRadius;
};

const uint MAX_NUMBER_OF_POINT_LIGHTS = 128;
uniform PointLight u_pointLights[MAX_NUMBER_OF_POINT_LIGHTS];
uniform uint u_pointLightNumber;

uniform vec3 u_viewPosition;

// OUTPUTS

layout(location = 0) out vec4 o_fragColor;

// CONSTANTS

const vec3 LIGHT_AMBIENT = vec3(0.02f);

// GLOBAL DATA

vec3 g_normal;
vec3 g_viewDirection;
vec3 g_diffuseMaterialValue;
vec3 g_specularMaterialValue;

// FUNCTIONS

vec3 getNormalisedSurfaceNormal()
{
	if (u_material.useNormalMap)
	{
		vec4 sampleFromNormalMap = texture(u_material.normalMap, vertex_output.textureCoordinates);
		vec3 sampledNormal = (sampleFromNormalMap.rgb * 2.0f) - 1.0f;
		vec3 sampledNormalInWorldSpace = vertex_output.TBN * sampledNormal;
		return normalize(sampledNormalInWorldSpace);
	}
	else
	return normalize(vertex_output.normal);
}

float calculatePointLightAttenuationFactor(float lightDistance, float lightRadius)
{
	const float lightSize = 0.01f;
	const uint n = 4;
	
	// Restrict the minimum value of the denominator to 0.01 * 0.01 to avoid the value
	// exploding or having divide by zero errors
	float inverseSquaredDistance = 1.0f / pow(max(lightDistance, lightSize), 2.0f);
	
	// Use a windowing function to cutoff the attenuation value to 0 at large distances
	
	float lightDistanceNOverLightRadiusN = 1.0f - pow(lightDistance / lightRadius, n);
	float windowingFunctionValue = pow(clamp(lightDistanceNOverLightRadiusN, 0.0f, 1.0f), 2.0f);
	
	return min(inverseSquaredDistance * windowingFunctionValue, 1.0f);
}

vec3 calculateDiffuseContribution(vec3 lightDirection, vec3 attenuatedLightDiffuseComponent)
{
	float lightIncidentDiffuseFactor = max(dot(lightDirection, g_normal), 0.0f);
	return (g_diffuseMaterialValue * lightIncidentDiffuseFactor) * attenuatedLightDiffuseComponent;
}

vec3 calculateSpecularContribution(vec3 lightDirection, vec3 attenuatedLightSpecularComponent)
{
	vec3 halfVector = normalize(g_viewDirection + lightDirection);
	float lightReflectedSpecularFactor = pow(max(dot(halfVector, g_normal), 0.0f), u_material.shininess);
	return (g_specularMaterialValue * lightReflectedSpecularFactor) * attenuatedLightSpecularComponent;
}

vec3 gammaCorrectColor(vec3 color)
{
	vec3 SRGBEncodedHigher = (1.055f * pow(color, vec3(1.0f / 2.4f))) - 0.055f;
	vec3 SRGBEncodedLower = 12.92f * color;
	float rSRGBEncoded = (color.r > 0.0031308f) ? SRGBEncodedHigher.r : SRGBEncodedLower.r;
	float gSRGBEncoded = (color.g > 0.0031308f) ? SRGBEncodedHigher.g : SRGBEncodedLower.g;
	float bSRGBEncoded = (color.b > 0.0031308f) ? SRGBEncodedHigher.b : SRGBEncodedLower.b;
	return vec3(rSRGBEncoded, gSRGBEncoded, bSRGBEncoded);
}

void main()
{
	vec3 color;
	g_normal = getNormalisedSurfaceNormal();
	g_viewDirection = normalize(u_viewPosition - vertex_output.worldPosition);
	
	vec4 diffuseMaterialValueWithAlpha = texture(u_material.diffuseMap, vertex_output.textureCoordinates) * u_material.diffuseColor;
	float alpha = diffuseMaterialValueWithAlpha.a;
	
	g_diffuseMaterialValue = diffuseMaterialValueWithAlpha.rgb;
	g_specularMaterialValue = texture(u_material.specularMap, vertex_output.textureCoordinates).rgb * u_material.specularColor;
	
	// Calculate ambient contribution
	
	color = g_diffuseMaterialValue * LIGHT_AMBIENT;
	
	// Calculate diffuse and specular contribution for each light
	
	for (uint i = 0; i < u_pointLightNumber; i++)
	{
		PointLight pointLight = u_pointLights[i];
		vec3 lightDirection = normalize(pointLight.worldPosition - vertex_output.worldPosition);
		
		float lightDistance = length(pointLight.worldPosition - vertex_output.worldPosition);
		float attenuationFactor = calculatePointLightAttenuationFactor(lightDistance, pointLight.lightRadius);
		
		color += calculateDiffuseContribution(lightDirection, pointLight.diffuseComponent * attenuationFactor);
		color += calculateSpecularContribution(lightDirection, pointLight.specularComponent * attenuationFactor);
	}
	
	o_fragColor = vec4(gammaCorrectColor(color), alpha);
}
\end{minted}

\section{Physically Based Shaders}

\subsection{PBS Vertex Shader}

\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
linenos,
breaklines
]
{glsl}
#version 460 core

// ATTRIBUTES

layout (location = 0) in vec3 a_position;
layout (location = 1) in vec3 a_normal;
layout (location = 2) in vec2 a_textureCoordinates;
layout (location = 3) in vec3 a_tangent;
layout (location = 4) in vec3 a_bitangent;

// UNIFORMS

uniform mat4 u_transform;
uniform mat4 u_projectionViewMatrix;

// OUTPUTS

struct VertexOutput
{
	vec3 worldPosition;
	vec3 normal;
	vec2 textureCoordinates;
	mat3 TBN;
};

out VertexOutput vertex_output;

// FUNCTIONS

void main()
{
	vertex_output.worldPosition = vec3(u_transform * vec4(a_position, 1.0f));
	vertex_output.normal = normalize(vec3(transpose(inverse(u_transform)) * vec4(a_normal, 0.0f)));
	vertex_output.textureCoordinates = a_textureCoordinates;
	
	vec3 normalTransformed = normalize(vec3(u_transform * vec4(a_normal, 0.0f)));
	vec3 tangentTransformed = normalize(vec3(u_transform * vec4(a_tangent, 0.0f)));
	vec3 bitangentTransformed = normalize(vec3(u_transform * vec4(a_bitangent, 0.0f)));
	vertex_output.TBN = mat3(tangentTransformed, bitangentTransformed, normalTransformed);
	
	gl_Position = u_projectionViewMatrix * u_transform * vec4(a_position, 1.0f);
}
\end{minted}

\subsection{PBS Fragment Shader}

\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
linenos,
breaklines
]
{glsl}
#version 460 core

// INPUTS FROM VERTEX SHADER

struct VertexOutput
{
	vec3 worldPosition;
	vec3 normal;
	vec2 textureCoordinates;
	mat3 TBN;
};

in VertexOutput vertex_output;

// UNIFORMS

// Material

struct Material
{
	vec4 baseColor;
	float roughness;
	float metalness;
	sampler2D baseColorMap;
	sampler2D roughnessMap;
	sampler2D metalnessMap;
	sampler2D normalMap;
	
	bool useNormalMap;
};

uniform Material u_material;

// Point lights

struct PointLight
{
	vec3 worldPosition;
	vec3 lightColor;
	float luminousPower;
	float lightRadius;
};

const uint MAX_NUMBER_OF_POINT_LIGHTS = 128;
uniform PointLight u_pointLights[MAX_NUMBER_OF_POINT_LIGHTS];
uniform uint u_pointLightNumber;

uniform vec3 u_viewPosition;

uniform float u_exposure;

// OUTPUTS

layout(location = 0) out vec4 o_fragColor;

// CONSTANTS

const vec3 F0_FOR_DIELECTRICS = vec3(0.04f);
const float PI = 3.14159265359;

// GLOBAL DATA

struct Directions
{
	vec3 normal;
	vec3 viewDirection;
};

Directions g_directions;

struct DotProducts
{
	float nDotV;
};

DotProducts g_dotProducts;

struct MaterialProperties
{
	vec3 baseColor;
	float alpha;
	float roughness;
	float metalness;
	
	vec3 f0;
};

MaterialProperties g_materialProperties;

// FUNCTIONS

/*
Used the Sclick approximation to calculate the Fresnel reflectance
*/
vec3 calculateFresnelSchlickApproximation(vec3 f0, float u)
{
	return f0 + (1 - f0) * pow((1 - u), 5.0f);
}

/*
Used the Smith height-correlated masking-shadowing function.

As pointed out by Lagarde, using a combination of Smith and the GGX NDF
in the specular (surface reflection) BRDF term means optimisations can be made.

Namely, G(l,v) / (4 * |n.l| * |n.v|) can be simplified. Hammon gives an accurate 
approximation for the above term. This approximation is being calculated in the function.

See https://www.gdcvault.com/play/1024478/PBR-Diffuse-Lighting-for-GGX
*/
float calculateHammonSmithMaskingSpecularDenominatorAppoximation(float nDotL)
{
	float alpha = g_materialProperties.roughness * g_materialProperties.roughness;
	float x = 2.0f * abs(nDotL) * abs(g_dotProducts.nDotV);
	float y = abs(nDotL) + abs(g_dotProducts.nDotV);
	return 1.0f / (2.0f * mix(x, y, alpha));
}

/*
Use the GGX (Trowbridge-Reitz) distribution for the NDF.

Also used the Disney mapping of alpha = roughness * roughness, where roughness
then gives a perceptually linear change from [0, 1].
*/
float calculateGGXDistribution(float nDotH)
{
	float alpha = g_materialProperties.roughness * g_materialProperties.roughness;
	float alpha2 = alpha * alpha;
	float x = 1 + (nDotH * nDotH * (alpha2 - 1.0f));
	return alpha2 / (PI * x * x);
}

vec3 getNormalisedSurfaceNormal()
{
	if (u_material.useNormalMap)
	{
		vec4 sampleFromNormalMap = texture(u_material.normalMap, vertex_output.textureCoordinates);
		vec3 sampledNormal = (sampleFromNormalMap.rgb * 2.0f) - 1.0f;
		vec3 sampledNormalInWorldSpace = vertex_output.TBN * sampledNormal;
		return normalize(sampledNormalInWorldSpace);
	}
	else
	return normalize(vertex_output.normal);
}

float calculatePointLightAttenuationFactor(float lightDistance, float lightRadius)
{
	const float lightSize = 0.01f;
	const uint n = 4;
	
	// Restrict the minimum value of the denominator to 0.01 * 0.01 to avoid the value
	// exploding or having divide by zero errors
	float inverseSquaredDistance = 1.0f / pow(max(lightDistance, lightSize), 2.0f);
	
	// Use a windowing function to cutoff the attenuation value to 0 at large distances
	
	float lightDistanceNOverLightRadiusN = 1.0f - pow(lightDistance / lightRadius, n);
	float windowingFunctionValue = pow(clamp(lightDistanceNOverLightRadiusN, 0.0f, 1.0f), 2.0f);
	
	return min(inverseSquaredDistance * windowingFunctionValue, 1.0f);
}

vec3 calculatePointLightContribution(const PointLight pointLight)
{
	// Calculate the incoming radiance from the point light
	
	float lightDistance = length(pointLight.worldPosition - vertex_output.worldPosition);
	float lightAttenuationFactor = calculatePointLightAttenuationFactor(lightDistance, pointLight.lightRadius);
	
	float luminousIntensity = pointLight.luminousPower / (4.0f * PI);
	vec3 lightRadiance = pointLight.lightColor * luminousIntensity * lightAttenuationFactor;
	
	// Initialise values
	
	vec3 lightDirection = normalize(pointLight.worldPosition - vertex_output.worldPosition);
	vec3 halfVector = normalize(g_directions.viewDirection + lightDirection);
	
	float nDotL = dot(g_directions.normal, lightDirection);
	float hDotL = dot(halfVector, lightDirection);
	float nDotH = dot(g_directions.normal, halfVector);
	
	// Specular (surface reflection) term
	
	// fresnelReflectance is also the specularTermContribution
	vec3 fresnelReflectance = calculateFresnelSchlickApproximation(g_materialProperties.f0, max(hDotL, 0.0f));
	float hammonSmithMaskingSpecularDenominatorApproximation = calculateHammonSmithMaskingSpecularDenominatorAppoximation(nDotL);
	float NDF = calculateGGXDistribution(nDotH);
	
	vec3 specularTerm = fresnelReflectance * hammonSmithMaskingSpecularDenominatorApproximation * NDF;
	
	// Diffuse (sub-surface reflection) term
	
	vec3 diffuseTermContribution = (vec3(1.0f) - fresnelReflectance) * (1.0f - g_materialProperties.metalness);
	vec3 diffuseTerm = diffuseTermContribution * (g_materialProperties.baseColor / PI);
	
	// Integrate the reflectance equation with respect to this light
	
	vec3 BRDFValue = diffuseTerm + specularTerm;
	return BRDFValue * lightRadiance * max(nDotL, 0.0f);
}

void main()
{
	// Initialise global values
	
	g_directions.normal = getNormalisedSurfaceNormal();
	g_directions.viewDirection = normalize(u_viewPosition - vertex_output.worldPosition);
	
	g_dotProducts.nDotV = dot(g_directions.normal, g_directions.viewDirection);
	
	vec4 baseColorWithAlpha = texture(u_material.baseColorMap, vertex_output.textureCoordinates) * u_material.baseColor;
	g_materialProperties.baseColor = baseColorWithAlpha.rgb;
	g_materialProperties.alpha = baseColorWithAlpha.a;
	g_materialProperties.roughness = texture(u_material.roughnessMap, vertex_output.textureCoordinates).r * u_material.roughness;
	g_materialProperties.metalness = texture(u_material.metalnessMap, vertex_output.textureCoordinates).r * u_material.metalness;
	g_materialProperties.f0 = mix(F0_FOR_DIELECTRICS, g_materialProperties.baseColor, g_materialProperties.metalness);
	
	// Solve the reflectance equation by evaluating the contribution of each point light
	
	vec3 fragmentColor = vec3(0.0f);
	
	for (uint i = 0; i < u_pointLightNumber; i++)
	fragmentColor += calculatePointLightContribution(u_pointLights[i]);
	
	// Apply a rudimentary ambient term
	
	vec3 ambientTerm = vec3(0.05f) * g_materialProperties.baseColor;
	fragmentColor += ambientTerm;
	
	// Output the shaded color
	
	o_fragColor = vec4(fragmentColor, g_materialProperties.alpha);
}
\end{minted}

\subsection{Post Processing Vertex Shader}

\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
linenos,
breaklines
]
{glsl}
#version 460 core

// ATTRIBUTES

layout (location = 0) in vec2 a_position;
layout (location = 1) in vec2 a_textureCoordinates;

// OUTPUTS

struct VertexOutput
{
	vec2 textureCoordinates;
};

out VertexOutput vertex_output;

// FUNCTIONS

void main()
{
	vertex_output.textureCoordinates = a_textureCoordinates;
	gl_Position = vec4(a_position, 0.0f, 1.0f);
}
\end{minted}

\subsection{Post Processing Fragment Shader}

\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
linenos,
breaklines
]
{glsl}
#version 460 core

// INPUTS FROM VERTEX SHADER

struct VertexOutput
{
	vec2 textureCoordinates;
};

in VertexOutput vertex_output;

// UNIFORMS

uniform sampler2D u_inputTexture;
uniform float u_exposure;

// OUTPUTS

layout(location = 0) out vec4 o_fragColor;

// FUNCTIONS

/*
ACES Tone Mapping

Curve adapted from: https://github.com/TheRealMJP/BakingLab/blob/master/BakingLab/ACES.hlsl
*/
vec3 applyToneMapping(vec3 color)
{
	mat3 inputMatrix = mat3
	(
		0.59719f,  0.07600f,  0.02840f,
		0.35458f,  0.90834f,  0.13383f,
		0.04823f,  0.01566f,  0.83777f
	);
	
	mat3 outputMatrix = mat3
	(
		1.60475f, -0.10208f, -0.00327f,
		-0.53108f,  1.10813f, -0.07276f,
		-0.07367f, -0.00605f,  1.07602f
	);
	
	color = inputMatrix * color;
	vec3 a = color * (color + 0.0245786f) - 0.000090537f;
	vec3 b = color * (0.983729f * color + 0.4329510f) + 0.238081f;
	color = a / b;
	
	return clamp(outputMatrix * color, 0.0f, 1.0f);
}

vec3 gammaCorrectColor(vec3 color)
{
	vec3 SRGBEncodedHigher = (1.055f * pow(color, vec3(1.0f / 2.4f))) - 0.055f;
	vec3 SRGBEncodedLower = 12.92f * color;
	float rSRGBEncoded = (color.r > 0.0031308f) ? SRGBEncodedHigher.r : SRGBEncodedLower.r;
	float gSRGBEncoded = (color.g > 0.0031308f) ? SRGBEncodedHigher.g : SRGBEncodedLower.g;
	float bSRGBEncoded = (color.b > 0.0031308f) ? SRGBEncodedHigher.b : SRGBEncodedLower.b;
	return vec3(rSRGBEncoded, gSRGBEncoded, bSRGBEncoded);
}

void main()
{
	vec4 inputColor = texture(u_inputTexture, vertex_output.textureCoordinates);
	float alpha = inputColor.a;
	
	// Apply exposure, tone mapping and gamma correction
	
	inputColor *= u_exposure;
	
	o_fragColor = vec4(gammaCorrectColor(applyToneMapping(inputColor.rgb)), alpha);
}
\end{minted}

\end{appendices}
